---
title: "Homework 10: Build a Shiny application"
author: "Soo Wan Kim"
date: "March 12, 2017"
output:
  html_document:
    keep_md: true
    theme: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE, cache = TRUE)

library(knitr)
```

#Kim Jong Un Going to Places:
##Tracking Kim Jong Un's Visits around North Korea

The purpose of this R Markdown file is to show how I obtained the data used in the *Kim Jong Un Going to Places* Shiny app.

**Note**: It takes 2-4 minutes to run the entire code for the first time.

![](http://www.euronews.com/media/download/articlepix/recadree/north-korea-tensions-kim-binoculars-08031.jpg)

## Data Sources

Information on dates and locations of Kim Jong Un's visits, and links to relevant articles and pictures were scraped from the website of North Korea's official state newspaper, [Rodong Sinmun](http://www.rodong.rep.kp/en/). Exact locations, when known, were gathered via web search. Geographic coordinates for the locations were retrieved using the Google Maps API, accessed using code by [Jose Gonzalez at R-bloggers](https://www.r-bloggers.com/using-google-maps-api-and-r/).

## Packages

I used the `rvest` package to scrape the Rodong Sinmun website and the `RJSONIO` and `RCurl` packages to access the Google Maps API.

```{r packages}
library(tidyverse)
library(rvest)
library(RJSONIO)
library(RCurl)
```

## Scraping Rodong Sinmun Part 1: Article Information

As of March 11, 2017, the section titled "Supreme Leader's Activity" had a total of 173 articles posted since January 2, 2016. The code below retrieves the article IDs, titles, and post dates and compiles them in a tidy dataset.

```{r scrape_article_info}
menu_url_base <- "http://rodong.rep.kp/en/index.php?strPageID=SF01_01_02&iMenuID=1&iSubMenuID="

#function to gather article IDs, titles, and post dates from each page
article_info <- function(i) {
  menu_url <- paste0(menu_url_base, as.character(i)) #get page url
  activities <- read_html(menu_url) #connect to url
  
  #scrape info and store in data frame
  news <- activities %>%
    html_nodes(".ListNewsLineContainer") %>%
    html_text() %>%
    as.data.frame()
  
  colnames(news) <- "news"
  
  #separate string into titles and dates
  news <- separate(news, news, into = c("title", "date"), sep = "\\[")
  #separate title string into IDs and titles
  news <- separate(news, title, into=c("id", "title"), sep = "\\.")
  
  #remove unnecessary characters from date string
  news$date <- gsub("\\]", "", news$date)
  news$date <- gsub("\\.", "-", news$date)
  
  return(news)
}

#initialize data frame
news <- data.frame(matrix(vector(), 0, 3,
                               dimnames=list(c(), c("id", "title", "date"))),
                        stringsAsFactors=F)

#bind data from each page into one data frame
for (i in 1:7) {
  news <- rbind(news, article_info(i))
}

#fix observations where the separate function got rid of part of the title due to a "." being present
news$title <- gsub("Farm No", "Farm No. 1116", news$title)
news$title <- gsub("Paektusan Hero Youth Power Station No", 
                        "Paektusan Hero Youth Power Station No. 3", news$title)

#convert date strings to date objects
news$date <- as.Date(news$date)

#clean up stray spaces behind article titles
news$title <- gsub(" $", "", news$title)
```

The code below filters the dataset to remove articles where Kim Jong Un is not visiting places but is carrying out some other activity, e.g. getting his picture taken.

```{r filter_article_info}
news_filt <- news %>%
  na.omit() %>%
  filter(grepl("^Kim Jong Un ", title, perl=TRUE)) %>%
  filter(!grepl("Has Photo Session", title, perl=TRUE)) %>%
  filter(!grepl("Kim Jong Un Makes", title, perl=TRUE)) %>%
  filter(!grepl("Kim Jong Un Meets", title, perl=TRUE)) %>%
  filter(!grepl("Kim Jong Un Issues Order", title, perl=TRUE)) 
```

The code below creates a new column where I try to isolate the visit site from the article title as much as possible. This makes it easier to compile the exact locations. 

```{r loc_clean}
news_filt$loc <- news_filt$title
news_filt$loc <- gsub("Kim Jong Un Visits", "", news_filt$loc)
news_filt$loc <- gsub("Kim Jong Un Inspects", "", news_filt$loc)
news_filt$loc <- gsub("Kim Jong Un Provides Field Guidance to", "", news_filt$loc)
news_filt$loc <- gsub("Kim Jong Un Gives Field Guidance to", "", news_filt$loc)
news_filt$loc <- gsub("Newly Built", "", news_filt$loc)
news_filt$loc <- gsub("Newly-Built", "", news_filt$loc)
news_filt$loc <- gsub("Newly-built", "", news_filt$loc)
news_filt$loc <- gsub("under Construction", "", news_filt$loc)
news_filt$loc <- gsub("to Express Condolences over Demise of Fidel Castro Ruz ", 
                      "", news_filt$loc)

#remove stray spaces at the start or end of location names
news_filt$loc <- gsub("^ ", "", news_filt$loc)
news_filt$loc <- gsub(" $", "", news_filt$loc)
```

## Getting Exact Locations

I used `write.csv(news_filt, "news_filt.csv")` to write the data frame into a `.csv` file, and entered information on the exact locations (name of each site, city and/or province) manually after googling the rough location names under the `loc` column.

## Getting Geographic Coordinates 

The code below imports the new file containing exact locations, joins it to the old data set, uses the Google Maps API to get the latitude and longtitude for each location, and finally merges everything into one dataframe.

```{r geo_coords}
#read in location data
loc_names <- read.csv("news_filt.csv") 
#join with old dataset
loc_names$id <- as.character(loc_names$id)
news_loc <- left_join(news_filt, loc_names, by="id")

#Use Google Maps API to get geographic coordinates of each place Kim Jong Un visited
##functions borrowed from here: https://www.r-bloggers.com/using-google-maps-api-and-r/

url <- function(address, return.call = "json", sensor = "false") {
  root <- "http://maps.google.com/maps/api/geocode/"
  u <- paste(root, return.call, "?address=", address, "&sensor=", sensor, sep = "")
  return(URLencode(u))
}

geoCode <- function(address,verbose=FALSE) {
  if(verbose) cat(address,"\n")
  u <- url(address)
  doc <- getURL(u)
  x <- fromJSON(doc,simplify = FALSE)
  if(x$status=="OK") {
    lat <- x$results[[1]]$geometry$location$lat
    lng <- x$results[[1]]$geometry$location$lng
    location_type <- x$results[[1]]$geometry$location_type
    formatted_address <- x$results[[1]]$formatted_address
    return(c(lat, lng, location_type, formatted_address))
  } else {
    return(c(NA,NA,NA, NA))
  }
}

#store coordinates in new data frame
geo <- map(news_loc$loc_geo, geoCode)
geo_df <- as.data.frame(geo) %>%
  t()
colnames(geo_df) <- c("lat", "lon", "prec", "loc2")
geo_df <- as_tibble(geo_df) %>%
  select(-prec)

#bind everything into one data frame
news_loc <- cbind(news_loc, geo_df) %>%
  select(-loc, -loc2)
```

Last step: export the data into a `.csv` file.

```{r loc_export}
write.csv(news_loc, "kju_visits_loc.csv")
```

## Scraping Rodong Sinmun Part 2: Links to Articles and Photos

The code chunk below retrieves the links to all articles and associated pictures from "Supreme Leader's Activity."

```{r scrape_article_links}
#function to gather links for each article

article_links <- function(i) {
  menu_url <- paste0(menu_url_base,as.character(i)) #get page url
  activities <- read_html(menu_url) #connect to page
  
  #scrape links
  links <- html_attr(html_nodes(activities, "a"), "href") %>% 
    as.data.frame()
  colnames(links) <- "link"
  
  #remove unnecessary characters
  links <- links %>%
    filter(grepl("javascript:article_open", link, perl=TRUE))
  links$link <- gsub("javascript:article_open\\('", "", links$link)
  links$link <- gsub("'\\)", "", links$link)
  
  #add identifying variables
  links <- links %>%
    mutate(date = substr(link, 39,48)) %>% #date
    mutate(date_id = substr(link, 39,53)) %>% #unique date-based id
    mutate(type = ifelse(grepl("_photo", link, perl=TRUE), #link to picture or article
                         "pic_link", "article_link")) %>%
    spread(type, link) %>%
    na.omit()
  
  return(links)
}

#initialize data frame
links <- data.frame(matrix(vector(), 0, 4,
                               dimnames=list(c(), c("date", "date_id", "article_link", "pic_link"))),
                        stringsAsFactors=F)

#bind data from each page into one data frame
for (i in 1:7) {
  links <- rbind(links, article_links(i))
}

#convert date strings to date objects
links$date <- as.Date(links$date)
```

Now, filter out irrelevant links, i.e. for articles that are not about Kim Jong Un going to places.

```{r filter_links}
#filter out links where the date does not match any of the dates in news_filt
links_filt <- links %>%
  filter(date %in% news_filt$date)

#identify links where date is not unique, i.e.
#multiple articles posted on same day

links_filt_mult <- links_filt %>%
  group_by(date) %>%
  summarize(count = n()) %>%
  filter(count > 1)

#for each date where multiple articles were posted, go through each article 
#to find the title

#articles to check:

links_check <- links_filt %>%
  filter(date %in% links_filt_mult$date)

#use a function to go through each article link in links_check
#and retrieve the article title

article_url_base <- "http://rodong.rep.kp/en/"

article_title <- function(link_bit) {
  article_link <- paste0(article_url_base, link_bit) #url
  article <- read_html(article_link) #connect to page
  
  #get article text
  text <- article %>%
    html_nodes(".ArticleContent") %>% 
    html_text()
  
  #get title
  title <- text[1]
  
  #make function slower to avoid crashing the site
  Sys.sleep(1.5)
  
  return(title)
}

#list of titles of articles in links_check
titles <- map(links_check$article_link, article_title)
#turn into dataframe
titles_df <- titles %>%
  as.data.frame() %>%
  t()

#bind titles to links_check
links_check <- cbind(links_check, titles_df)
links_check$titles_df <- as.character(links_check$titles_df)

#isolate links to remove based on the titles
links_rm <- links_check %>%
  filter(!(titles_df %in% news_filt$title)) %>%
  rename(title = titles_df)

#filter list of links to include only links to relevant articles
links_filt2 <- anti_join(links_filt, links_rm) %>%
  arrange(date)
```

Export the links into a `.csv` file.

```{r links_export}
write.csv(links_filt2, "kju_visits_links.csv")
```

```{r session_info, include=FALSE}
devtools::session_info()
```